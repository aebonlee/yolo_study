{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì„±ëŠ¥ ìµœì í™” ë° GPU ê°€ì† ì„¤ì •\n",
    "\n",
    "**ëª©ì **: YOLO11 ë“œë¡  ì‘ë¬¼ íƒì§€ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ ìµœì í™”  \n",
    "**ë‹´ë‹¹**: Claude Sonnet 4  \n",
    "**ë‚ ì§œ**: 2025-10-21\n",
    "\n",
    "## ğŸ“‹ ì‘ì—… ë‚´ìš©\n",
    "1. GPU ìµœì í™” ë° ë©”ëª¨ë¦¬ ê´€ë¦¬\n",
    "2. ëª¨ë¸ ìµœì í™” (TensorRT, ONNX ë³€í™˜)\n",
    "3. ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”\n",
    "4. ë©”ëª¨ë¦¬ í’€ë§ ë° ìºì‹±\n",
    "5. í”„ë¡œíŒŒì¼ë§ ë° ë³‘ëª©ì  ë¶„ì„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° GPU ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import psutil\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import deque\n",
    "import json\n",
    "import pickle\n",
    "import hashlib\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# GPU ìµœì í™” ì„¤ì •\n",
    "if torch.cuda.is_available():\n",
    "    # CUDA ìµœì í™” ì„¤ì •\n",
    "    torch.backends.cudnn.benchmark = True  # ê³ ì •ëœ ì…ë ¥ í¬ê¸°ì— ëŒ€í•´ ìµœì í™”\n",
    "    torch.backends.cudnn.deterministic = False  # ì„±ëŠ¥ ìš°ì„ \n",
    "    torch.backends.cuda.matmul.allow_tf32 = True  # TF32 ì‚¬ìš© (A100+)\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA ë²„ì „: {torch.version.cuda}\")\n",
    "    print(f\"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"cuDNN ë²„ì „: {torch.backends.cudnn.version()}\")\n",
    "    \n",
    "    # GPU ìµœì í™” ìƒíƒœ í™•ì¸\n",
    "    print(f\"\\nğŸ”§ GPU ìµœì í™” ì„¤ì •:\")\n",
    "    print(f\"   cuDNN benchmark: {torch.backends.cudnn.benchmark}\")\n",
    "    print(f\"   TF32 matmul: {torch.backends.cuda.matmul.allow_tf32}\")\n",
    "    print(f\"   TF32 cuDNN: {torch.backends.cudnn.allow_tf32}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUMemoryManager:\n",
    "    \"\"\"GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, device: str = 'cuda', cache_limit: float = 0.8):\n",
    "        \"\"\"ì´ˆê¸°í™”\n",
    "        \n",
    "        Args:\n",
    "            device: GPU ë””ë°”ì´ìŠ¤\n",
    "            cache_limit: ìºì‹œ ë©”ëª¨ë¦¬ ì œí•œ (ì „ì²´ ë©”ëª¨ë¦¬ ëŒ€ë¹„ ë¹„ìœ¨)\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.cache_limit = cache_limit\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "            self.max_cache_memory = int(self.total_memory * cache_limit)\n",
    "            \n",
    "            # ë©”ëª¨ë¦¬ í• ë‹¹ ì „ëµ ì„¤ì •\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        else:\n",
    "            self.total_memory = 0\n",
    "            self.max_cache_memory = 0\n",
    "        \n",
    "        print(f\"ğŸ”§ GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”\")\n",
    "        print(f\"   ì´ ë©”ëª¨ë¦¬: {self.total_memory / 1e9:.1f} GB\")\n",
    "        print(f\"   ìºì‹œ ì œí•œ: {self.max_cache_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    def get_memory_info(self) -> Dict:\n",
    "        \"\"\"ë©”ëª¨ë¦¬ ì •ë³´ ë°˜í™˜\n",
    "        \n",
    "        Returns:\n",
    "            memory_info: ë©”ëª¨ë¦¬ ì •ë³´\n",
    "        \"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return {'total': 0, 'allocated': 0, 'cached': 0, 'free': 0}\n",
    "        \n",
    "        allocated = torch.cuda.memory_allocated()\n",
    "        cached = torch.cuda.memory_reserved()\n",
    "        peak_allocated = torch.cuda.max_memory_allocated()\n",
    "        free = self.total_memory - cached\n",
    "        \n",
    "        return {\n",
    "            'total': self.total_memory,\n",
    "            'allocated': allocated,\n",
    "            'cached': cached,\n",
    "            'peak_allocated': peak_allocated,\n",
    "            'free': free,\n",
    "            'utilization': allocated / self.total_memory if self.total_memory > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def optimize_memory(self):\n",
    "        \"\"\"ë©”ëª¨ë¦¬ ìµœì í™”\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "        \n",
    "        memory_info = self.get_memory_info()\n",
    "        \n",
    "        # ìºì‹œê°€ ì œí•œì„ ì´ˆê³¼í•˜ë©´ ì •ë¦¬\n",
    "        if memory_info['cached'] > self.max_cache_memory:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            print(f\"ğŸ“ˆ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: {memory_info['cached']/1e9:.1f}GB â†’ {torch.cuda.memory_reserved()/1e9:.1f}GB\")\n",
    "    \n",
    "    def set_memory_fraction(self, fraction: float = 0.9):\n",
    "        \"\"\"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ\n",
    "        \n",
    "        Args:\n",
    "            fraction: ì‚¬ìš©í•  ë©”ëª¨ë¦¬ ë¹„ìœ¨\n",
    "        \"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.set_per_process_memory_fraction(fraction)\n",
    "            print(f\"GPU ë©”ëª¨ë¦¬ ì œí•œ: {fraction*100:.1f}% ({self.total_memory*fraction/1e9:.1f}GB)\")\n",
    "    \n",
    "    def log_memory_usage(self, prefix: str = \"\"):\n",
    "        \"\"\"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…\n",
    "        \n",
    "        Args:\n",
    "            prefix: ë¡œê·¸ ì ‘ë‘ì‚¬\n",
    "        \"\"\"\n",
    "        info = self.get_memory_info()\n",
    "        print(f\"{prefix}GPU ë©”ëª¨ë¦¬: {info['allocated']/1e9:.2f}GB / {info['total']/1e9:.1f}GB ({info['utilization']*100:.1f}%)\")\n",
    "\n",
    "# GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”\n",
    "gpu_manager = GPUMemoryManager()\n",
    "gpu_manager.log_memory_usage(\"ì´ˆê¸° \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ëª¨ë¸ ìµœì í™” í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelOptimizer:\n",
    "    \"\"\"YOLO ëª¨ë¸ ìµœì í™” í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str = 'yolo11n.pt'):\n",
    "        \"\"\"ì´ˆê¸°í™”\n",
    "        \n",
    "        Args:\n",
    "            model_path: ëª¨ë¸ ê²½ë¡œ\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.model = YOLO(model_path)\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        # ìµœì í™”ëœ ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n",
    "        self.optimized_models_dir = Path('optimized_models')\n",
    "        self.optimized_models_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        print(f\"ğŸ¯ ModelOptimizer ì´ˆê¸°í™”\")\n",
    "        print(f\"   ê¸°ë³¸ ëª¨ë¸: {model_path}\")\n",
    "        print(f\"   ë””ë°”ì´ìŠ¤: {self.device}\")\n",
    "    \n",
    "    def optimize_for_inference(self) -> YOLO:\n",
    "        \"\"\"ì¶”ë¡  ìµœì í™”ëœ ëª¨ë¸ ìƒì„±\n",
    "        \n",
    "        Returns:\n",
    "            optimized_model: ìµœì í™”ëœ ëª¨ë¸\n",
    "        \"\"\"\n",
    "        print(\"ğŸ”„ ì¶”ë¡  ìµœì í™” ì‹œì‘...\")\n",
    "        \n",
    "        # PyTorch ìµœì í™”\n",
    "        optimized_model = self.model\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            # GPU ìµœì í™”\n",
    "            optimized_model.model = optimized_model.model.to(self.device)\n",
    "            optimized_model.model.eval()\n",
    "            \n",
    "            # ëª¨ë¸ì„ evaluation ëª¨ë“œë¡œ ì„¤ì •\n",
    "            for module in optimized_model.model.modules():\n",
    "                if hasattr(module, 'training'):\n",
    "                    module.training = False\n",
    "            \n",
    "            # ì¶”ê°€ ìµœì í™”\n",
    "            if hasattr(torch.jit, 'optimize_for_inference'):\n",
    "                try:\n",
    "                    # JIT ìµœì í™” (ê°€ëŠ¥í•œ ê²½ìš°)\n",
    "                    dummy_input = torch.randn(1, 3, 640, 640).to(self.device)\n",
    "                    traced_model = torch.jit.trace(optimized_model.model, dummy_input)\n",
    "                    traced_model = torch.jit.optimize_for_inference(traced_model)\n",
    "                    print(\"   âœ… JIT ìµœì í™” ì™„ë£Œ\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   âš ï¸ JIT ìµœì í™” ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "        print(\"âœ… ì¶”ë¡  ìµœì í™” ì™„ë£Œ\")\n",
    "        return optimized_model\n",
    "    \n",
    "    def export_to_onnx(self, input_size: Tuple[int, int] = (640, 640)) -> str:\n",
    "        \"\"\"ONNX í˜•ì‹ìœ¼ë¡œ ëª¨ë¸ ë‚´ë³´ë‚´ê¸°\n",
    "        \n",
    "        Args:\n",
    "            input_size: ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°\n",
    "            \n",
    "        Returns:\n",
    "            onnx_path: ONNX íŒŒì¼ ê²½ë¡œ\n",
    "        \"\"\"\n",
    "        print(\"ğŸ”„ ONNX ë³€í™˜ ì‹œì‘...\")\n",
    "        \n",
    "        try:\n",
    "            # ONNX íŒŒì¼ ê²½ë¡œ\n",
    "            model_name = Path(self.model_path).stem\n",
    "            onnx_path = self.optimized_models_dir / f\"{model_name}_optimized.onnx\"\n",
    "            \n",
    "            # YOLO ëª¨ë¸ì˜ ONNX ë‚´ë³´ë‚´ê¸° ê¸°ëŠ¥ ì‚¬ìš©\n",
    "            success = self.model.export(\n",
    "                format='onnx',\n",
    "                imgsz=input_size,\n",
    "                dynamic=False,  # ê³ ì • í¬ê¸°ë¡œ ìµœì í™”\n",
    "                simplify=True,   # ëª¨ë¸ ë‹¨ìˆœí™”\n",
    "                opset=12         # ONNX opset ë²„ì „\n",
    "            )\n",
    "            \n",
    "            if success:\n",
    "                print(f\"âœ… ONNX ë³€í™˜ ì™„ë£Œ: {onnx_path}\")\n",
    "                return str(onnx_path)\n",
    "            else:\n",
    "                print(\"âŒ ONNX ë³€í™˜ ì‹¤íŒ¨\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ONNX ë³€í™˜ ì˜¤ë¥˜: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def export_to_tensorrt(self, input_size: Tuple[int, int] = (640, 640)) -> str:\n",
    "        \"\"\"TensorRT í˜•ì‹ìœ¼ë¡œ ëª¨ë¸ ë‚´ë³´ë‚´ê¸°\n",
    "        \n",
    "        Args:\n",
    "            input_size: ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°\n",
    "            \n",
    "        Returns:\n",
    "            tensorrt_path: TensorRT íŒŒì¼ ê²½ë¡œ\n",
    "        \"\"\"\n",
    "        print(\"ğŸ”„ TensorRT ë³€í™˜ ì‹œì‘...\")\n",
    "        \n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"âŒ TensorRTëŠ” CUDAê°€ í•„ìš”í•©ë‹ˆë‹¤\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # TensorRT íŒŒì¼ ê²½ë¡œ\n",
    "            model_name = Path(self.model_path).stem\n",
    "            tensorrt_path = self.optimized_models_dir / f\"{model_name}_tensorrt.engine\"\n",
    "            \n",
    "            # YOLO ëª¨ë¸ì˜ TensorRT ë‚´ë³´ë‚´ê¸° ê¸°ëŠ¥ ì‚¬ìš©\n",
    "            success = self.model.export(\n",
    "                format='engine',  # TensorRT engine\n",
    "                imgsz=input_size,\n",
    "                dynamic=False,\n",
    "                half=True,        # FP16 ì •ë°€ë„\n",
    "                workspace=4       # ì‘ì—…ê³µê°„ í¬ê¸° (GB)\n",
    "            )\n",
    "            \n",
    "            if success:\n",
    "                print(f\"âœ… TensorRT ë³€í™˜ ì™„ë£Œ: {tensorrt_path}\")\n",
    "                return str(tensorrt_path)\n",
    "            else:\n",
    "                print(\"âŒ TensorRT ë³€í™˜ ì‹¤íŒ¨\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ TensorRT ë³€í™˜ ì˜¤ë¥˜: {e}\")\n",
    "            print(\"   TensorRTê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤\")\n",
    "            return None\n",
    "    \n",
    "    def benchmark_models(self, test_image: np.ndarray, iterations: int = 100) -> Dict:\n",
    "        \"\"\"ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬\n",
    "        \n",
    "        Args:\n",
    "            test_image: í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€\n",
    "            iterations: ë°˜ë³µ íšŸìˆ˜\n",
    "            \n",
    "        Returns:\n",
    "            benchmark_results: ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ”¥ ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ({iterations}íšŒ)\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # 1. ê¸°ë³¸ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬\n",
    "        print(\"\\nğŸ“Š ê¸°ë³¸ ëª¨ë¸:\")\n",
    "        base_times = []\n",
    "        \n",
    "        # ì›Œë°ì—…\n",
    "        for _ in range(5):\n",
    "            _ = self.model(test_image, device=self.device, verbose=False)\n",
    "        \n",
    "        # ë²¤ì¹˜ë§ˆí¬\n",
    "        gpu_manager.log_memory_usage(\"   ì‹œì‘ ì „ \")\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            start_time = time.time()\n",
    "            _ = self.model(test_image, device=self.device, verbose=False)\n",
    "            end_time = time.time()\n",
    "            base_times.append(end_time - start_time)\n",
    "            \n",
    "            if (i + 1) % 50 == 0:\n",
    "                avg_time = np.mean(base_times[-50:])\n",
    "                print(f\"   ì§„í–‰ë¥ : {i+1}/{iterations}, í‰ê· : {avg_time*1000:.1f}ms\")\n",
    "        \n",
    "        gpu_manager.log_memory_usage(\"   ì™„ë£Œ í›„ \")\n",
    "        \n",
    "        base_avg = np.mean(base_times)\n",
    "        base_std = np.std(base_times)\n",
    "        base_fps = 1.0 / base_avg\n",
    "        \n",
    "        results['base_model'] = {\n",
    "            'avg_time': base_avg,\n",
    "            'std_time': base_std,\n",
    "            'fps': base_fps,\n",
    "            'times': base_times\n",
    "        }\n",
    "        \n",
    "        print(f\"   í‰ê·  ì‹œê°„: {base_avg*1000:.2f} Â± {base_std*1000:.2f}ms\")\n",
    "        print(f\"   í‰ê·  FPS: {base_fps:.1f}\")\n",
    "        \n",
    "        # 2. ìµœì í™”ëœ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬\n",
    "        try:\n",
    "            print(\"\\nğŸ“Š ìµœì í™”ëœ ëª¨ë¸:\")\n",
    "            optimized_model = self.optimize_for_inference()\n",
    "            \n",
    "            opt_times = []\n",
    "            \n",
    "            # ì›Œë°ì—…\n",
    "            for _ in range(5):\n",
    "                _ = optimized_model(test_image, device=self.device, verbose=False)\n",
    "            \n",
    "            # ë²¤ì¹˜ë§ˆí¬\n",
    "            for i in range(iterations):\n",
    "                start_time = time.time()\n",
    "                _ = optimized_model(test_image, device=self.device, verbose=False)\n",
    "                end_time = time.time()\n",
    "                opt_times.append(end_time - start_time)\n",
    "                \n",
    "                if (i + 1) % 50 == 0:\n",
    "                    avg_time = np.mean(opt_times[-50:])\n",
    "                    print(f\"   ì§„í–‰ë¥ : {i+1}/{iterations}, í‰ê· : {avg_time*1000:.1f}ms\")\n",
    "            \n",
    "            opt_avg = np.mean(opt_times)\n",
    "            opt_std = np.std(opt_times)\n",
    "            opt_fps = 1.0 / opt_avg\n",
    "            \n",
    "            results['optimized_model'] = {\n",
    "                'avg_time': opt_avg,\n",
    "                'std_time': opt_std,\n",
    "                'fps': opt_fps,\n",
    "                'times': opt_times\n",
    "            }\n",
    "            \n",
    "            speedup = base_avg / opt_avg\n",
    "            print(f\"   í‰ê·  ì‹œê°„: {opt_avg*1000:.2f} Â± {opt_std*1000:.2f}ms\")\n",
    "            print(f\"   í‰ê·  FPS: {opt_fps:.1f}\")\n",
    "            print(f\"   ê°€ì† ë¹„ìœ¨: {speedup:.2f}x\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ ìµœì í™” ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# ëª¨ë¸ ìµœì í™” ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "model_optimizer = ModelOptimizer('yolo11n.pt')\n",
    "print(\"âœ… ModelOptimizer ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchProcessor:\n",
    "    \"\"\"ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model: YOLO, \n",
    "                 batch_size: int = 4,\n",
    "                 max_queue_size: int = 100):\n",
    "        \"\"\"ì´ˆê¸°í™”\n",
    "        \n",
    "        Args:\n",
    "            model: YOLO ëª¨ë¸\n",
    "            batch_size: ë°°ì¹˜ í¬ê¸°\n",
    "            max_queue_size: ìµœëŒ€ í í¬ê¸°\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.max_queue_size = max_queue_size\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        # ë°°ì¹˜ í\n",
    "        self.input_queue = deque(maxlen=max_queue_size)\n",
    "        self.output_queue = deque(maxlen=max_queue_size)\n",
    "        \n",
    "        # í†µê³„\n",
    "        self.processing_times = deque(maxlen=1000)\n",
    "        self.batch_counts = deque(maxlen=1000)\n",
    "        \n",
    "        print(f\"âš¡ BatchProcessor ì´ˆê¸°í™”\")\n",
    "        print(f\"   ë°°ì¹˜ í¬ê¸°: {batch_size}\")\n",
    "        print(f\"   í í¬ê¸°: {max_queue_size}\")\n",
    "        print(f\"   ë””ë°”ì´ìŠ¤: {self.device}\")\n",
    "    \n",
    "    def add_image(self, image: np.ndarray, metadata: Dict = None) -> int:\n",
    "        \"\"\"ì´ë¯¸ì§€ë¥¼ ë°°ì¹˜ íì— ì¶”ê°€\n",
    "        \n",
    "        Args:\n",
    "            image: ì…ë ¥ ì´ë¯¸ì§€\n",
    "            metadata: ë©”íƒ€ë°ì´í„°\n",
    "            \n",
    "        Returns:\n",
    "            image_id: ì´ë¯¸ì§€ ID\n",
    "        \"\"\"\n",
    "        image_id = len(self.input_queue)\n",
    "        \n",
    "        self.input_queue.append({\n",
    "            'id': image_id,\n",
    "            'image': image,\n",
    "            'metadata': metadata or {},\n",
    "            'timestamp': time.time()\n",
    "        })\n",
    "        \n",
    "        return image_id\n",
    "    \n",
    "    def process_batch(self) -> List[Dict]:\n",
    "        \"\"\"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰\n",
    "        \n",
    "        Returns:\n",
    "            results: ì²˜ë¦¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        if len(self.input_queue) == 0:\n",
    "            return []\n",
    "        \n",
    "        # ë°°ì¹˜ ìƒì„±\n",
    "        batch_items = []\n",
    "        batch_images = []\n",
    "        \n",
    "        for _ in range(min(self.batch_size, len(self.input_queue))):\n",
    "            if self.input_queue:\n",
    "                item = self.input_queue.popleft()\n",
    "                batch_items.append(item)\n",
    "                batch_images.append(item['image'])\n",
    "        \n",
    "        if not batch_images:\n",
    "            return []\n",
    "        \n",
    "        # ë°°ì¹˜ ì¶”ë¡ \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # YOLO ë°°ì¹˜ ì¶”ë¡ \n",
    "            results = self.model(\n",
    "                batch_images,\n",
    "                device=self.device,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            # ê²°ê³¼ ì²˜ë¦¬\n",
    "            batch_results = []\n",
    "            \n",
    "            for i, (item, result) in enumerate(zip(batch_items, results)):\n",
    "                detections = []\n",
    "                \n",
    "                if result.boxes is not None:\n",
    "                    for box in result.boxes:\n",
    "                        cls_id = int(box.cls)\n",
    "                        confidence = float(box.conf)\n",
    "                        bbox = box.xyxy[0].cpu().numpy()\n",
    "                        class_name = self.model.names[cls_id]\n",
    "                        \n",
    "                        detections.append({\n",
    "                            'class_id': cls_id,\n",
    "                            'class_name': class_name,\n",
    "                            'confidence': confidence,\n",
    "                            'bbox': bbox.tolist()\n",
    "                        })\n",
    "                \n",
    "                batch_result = {\n",
    "                    'id': item['id'],\n",
    "                    'detections': detections,\n",
    "                    'processing_time': processing_time / len(batch_items),\n",
    "                    'batch_size': len(batch_items),\n",
    "                    'timestamp': item['timestamp'],\n",
    "                    'metadata': item['metadata']\n",
    "                }\n",
    "                \n",
    "                batch_results.append(batch_result)\n",
    "                self.output_queue.append(batch_result)\n",
    "            \n",
    "            # í†µê³„ ì—…ë°ì´íŠ¸\n",
    "            self.processing_times.append(processing_time)\n",
    "            self.batch_counts.append(len(batch_items))\n",
    "            \n",
    "            return batch_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def process_all(self) -> List[Dict]:\n",
    "        \"\"\"íì˜ ëª¨ë“  ì´ë¯¸ì§€ ì²˜ë¦¬\n",
    "        \n",
    "        Returns:\n",
    "            all_results: ì „ì²´ ì²˜ë¦¬ ê²°ê³¼\n",
    "        \"\"\"\n",
    "        all_results = []\n",
    "        \n",
    "        while self.input_queue:\n",
    "            batch_results = self.process_batch()\n",
    "            all_results.extend(batch_results)\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"ë°°ì¹˜ ì²˜ë¦¬ í†µê³„\n",
    "        \n",
    "        Returns:\n",
    "            stats: í†µê³„ ì •ë³´\n",
    "        \"\"\"\n",
    "        if not self.processing_times:\n",
    "            return {'avg_batch_time': 0, 'avg_batch_size': 0, 'throughput': 0}\n",
    "        \n",
    "        avg_batch_time = np.mean(self.processing_times)\n",
    "        avg_batch_size = np.mean(self.batch_counts)\n",
    "        throughput = avg_batch_size / avg_batch_time  # images/sec\n",
    "        \n",
    "        return {\n",
    "            'avg_batch_time': avg_batch_time,\n",
    "            'avg_batch_size': avg_batch_size,\n",
    "            'throughput': throughput,\n",
    "            'total_batches': len(self.processing_times),\n",
    "            'queue_size': len(self.input_queue)\n",
    "        }\n",
    "    \n",
    "    def clear_queues(self):\n",
    "        \"\"\"í ì´ˆê¸°í™”\"\"\"\n",
    "        self.input_queue.clear()\n",
    "        self.output_queue.clear()\n",
    "\n",
    "# ë°°ì¹˜ ì²˜ë¦¬ê¸° ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)\n",
    "test_model = YOLO('yolo11n.pt')\n",
    "batch_processor = BatchProcessor(test_model, batch_size=4)\n",
    "print(\"âœ… BatchProcessor ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ë©”ëª¨ë¦¬ í’€ë§ ë° ìºì‹±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheManager:\n",
    "    \"\"\"ê²°ê³¼ ìºì‹± ë° ë©”ëª¨ë¦¬ í’€ë§ ê´€ë¦¬ì\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 max_cache_size: int = 1000,\n",
    "                 cache_ttl: float = 3600.0):  # 1ì‹œê°„\n",
    "        \"\"\"ì´ˆê¸°í™”\n",
    "        \n",
    "        Args:\n",
    "            max_cache_size: ìµœëŒ€ ìºì‹œ í¬ê¸°\n",
    "            cache_ttl: ìºì‹œ TTL (ì´ˆ)\n",
    "        \"\"\"\n",
    "        self.max_cache_size = max_cache_size\n",
    "        self.cache_ttl = cache_ttl\n",
    "        \n",
    "        # ê²°ê³¼ ìºì‹œ\n",
    "        self.result_cache = {}\n",
    "        self.cache_timestamps = {}\n",
    "        \n",
    "        # ì´ë¯¸ì§€ í•´ì‹œ ìºì‹œ\n",
    "        self.hash_cache = {}\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ í’€\n",
    "        self.tensor_pool = {}\n",
    "        self.array_pool = {}\n",
    "        \n",
    "        # í†µê³„\n",
    "        self.cache_hits = 0\n",
    "        self.cache_misses = 0\n",
    "        \n",
    "        print(f\"ğŸ’¾ CacheManager ì´ˆê¸°í™”\")\n",
    "        print(f\"   ìµœëŒ€ ìºì‹œ í¬ê¸°: {max_cache_size}\")\n",
    "        print(f\"   ìºì‹œ TTL: {cache_ttl}ì´ˆ\")\n",
    "    \n",
    "    def _compute_image_hash(self, image: np.ndarray) -> str:\n",
    "        \"\"\"ì´ë¯¸ì§€ í•´ì‹œ ê³„ì‚°\n",
    "        \n",
    "        Args:\n",
    "            image: ì…ë ¥ ì´ë¯¸ì§€\n",
    "            \n",
    "        Returns:\n",
    "            hash_str: ì´ë¯¸ì§€ í•´ì‹œ\n",
    "        \"\"\"\n",
    "        # ì´ë¯¸ì§€ë¥¼ ë¦¬ì‚¬ì´ì¦ˆí•˜ì—¬ í•´ì‹œ ê³„ì‚° (ì„±ëŠ¥ í–¥ìƒ)\n",
    "        small_image = cv2.resize(image, (64, 64))\n",
    "        image_bytes = small_image.tobytes()\n",
    "        return hashlib.md5(image_bytes).hexdigest()\n",
    "    \n",
    "    def get_cached_result(self, image: np.ndarray) -> Optional[Dict]:\n",
    "        \"\"\"ìºì‹œëœ ê²°ê³¼ ì¡°íšŒ\n",
    "        \n",
    "        Args:\n",
    "            image: ì…ë ¥ ì´ë¯¸ì§€\n",
    "            \n",
    "        Returns:\n",
    "            cached_result: ìºì‹œëœ ê²°ê³¼ ë˜ëŠ” None\n",
    "        \"\"\"\n",
    "        image_hash = self._compute_image_hash(image)\n",
    "        \n",
    "        # ìºì‹œ í™•ì¸\n",
    "        if image_hash in self.result_cache:\n",
    "            # TTL í™•ì¸\n",
    "            if time.time() - self.cache_timestamps[image_hash] < self.cache_ttl:\n",
    "                self.cache_hits += 1\n",
    "                return self.result_cache[image_hash]\n",
    "            else:\n",
    "                # ë§Œë£Œëœ ìºì‹œ ì œê±°\n",
    "                del self.result_cache[image_hash]\n",
    "                del self.cache_timestamps[image_hash]\n",
    "        \n",
    "        self.cache_misses += 1\n",
    "        return None\n",
    "    \n",
    "    def cache_result(self, image: np.ndarray, result: Dict):\n",
    "        \"\"\"ê²°ê³¼ ìºì‹±\n",
    "        \n",
    "        Args:\n",
    "            image: ì…ë ¥ ì´ë¯¸ì§€\n",
    "            result: íƒì§€ ê²°ê³¼\n",
    "        \"\"\"\n",
    "        image_hash = self._compute_image_hash(image)\n",
    "        \n",
    "        # ìºì‹œ í¬ê¸° ì œí•œ\n",
    "        if len(self.result_cache) >= self.max_cache_size:\n",
    "            self._evict_oldest_cache()\n",
    "        \n",
    "        # ê²°ê³¼ ì €ì¥\n",
    "        self.result_cache[image_hash] = result.copy()\n",
    "        self.cache_timestamps[image_hash] = time.time()\n",
    "    \n",
    "    def _evict_oldest_cache(self):\n",
    "        \"\"\"ê°€ì¥ ì˜¤ë˜ëœ ìºì‹œ ì œê±°\"\"\"\n",
    "        if not self.cache_timestamps:\n",
    "            return\n",
    "        \n",
    "        oldest_hash = min(self.cache_timestamps.keys(), \n",
    "                         key=lambda k: self.cache_timestamps[k])\n",
    "        \n",
    "        del self.result_cache[oldest_hash]\n",
    "        del self.cache_timestamps[oldest_hash]\n",
    "    \n",
    "    def get_tensor_from_pool(self, shape: Tuple, dtype: torch.dtype = torch.float32) -> torch.Tensor:\n",
    "        \"\"\"í…ì„œ í’€ì—ì„œ í…ì„œ ê°€ì ¸ì˜¤ê¸°\n",
    "        \n",
    "        Args:\n",
    "            shape: í…ì„œ ëª¨ì–‘\n",
    "            dtype: ë°ì´í„° íƒ€ì…\n",
    "            \n",
    "        Returns:\n",
    "            tensor: í’€ë§ëœ í…ì„œ\n",
    "        \"\"\"\n",
    "        key = (shape, dtype)\n",
    "        \n",
    "        if key in self.tensor_pool and self.tensor_pool[key]:\n",
    "            return self.tensor_pool[key].pop()\n",
    "        else:\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            return torch.empty(shape, dtype=dtype, device=device)\n",
    "    \n",
    "    def return_tensor_to_pool(self, tensor: torch.Tensor):\n",
    "        \"\"\"í…ì„œë¥¼ í’€ì— ë°˜í™˜\n",
    "        \n",
    "        Args:\n",
    "            tensor: ë°˜í™˜í•  í…ì„œ\n",
    "        \"\"\"\n",
    "        key = (tuple(tensor.shape), tensor.dtype)\n",
    "        \n",
    "        if key not in self.tensor_pool:\n",
    "            self.tensor_pool[key] = []\n",
    "        \n",
    "        # í’€ í¬ê¸° ì œí•œ\n",
    "        if len(self.tensor_pool[key]) < 10:\n",
    "            tensor.zero_()  # ë‚´ìš© ì´ˆê¸°í™”\n",
    "            self.tensor_pool[key].append(tensor)\n",
    "    \n",
    "    def get_cache_statistics(self) -> Dict:\n",
    "        \"\"\"ìºì‹œ í†µê³„ ë°˜í™˜\n",
    "        \n",
    "        Returns:\n",
    "            stats: ìºì‹œ í†µê³„\n",
    "        \"\"\"\n",
    "        total_requests = self.cache_hits + self.cache_misses\n",
    "        hit_rate = self.cache_hits / total_requests if total_requests > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'cache_size': len(self.result_cache),\n",
    "            'max_cache_size': self.max_cache_size,\n",
    "            'cache_hits': self.cache_hits,\n",
    "            'cache_misses': self.cache_misses,\n",
    "            'hit_rate': hit_rate,\n",
    "            'tensor_pool_sizes': {str(k): len(v) for k, v in self.tensor_pool.items()}\n",
    "        }\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"ìºì‹œ ì´ˆê¸°í™”\"\"\"\n",
    "        self.result_cache.clear()\n",
    "        self.cache_timestamps.clear()\n",
    "        self.hash_cache.clear()\n",
    "        \n",
    "        # í…ì„œ í’€ ì •ë¦¬\n",
    "        for tensor_list in self.tensor_pool.values():\n",
    "            for tensor in tensor_list:\n",
    "                del tensor\n",
    "        self.tensor_pool.clear()\n",
    "        \n",
    "        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# ìºì‹œ ê´€ë¦¬ì ì´ˆê¸°í™”\n",
    "cache_manager = CacheManager(max_cache_size=500)\n",
    "print(\"âœ… CacheManager ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. í†µí•© ì„±ëŠ¥ ìµœì í™” í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedCropDetector:\n",
    "    \"\"\"ìµœì í™”ëœ ë“œë¡  ì‘ë¬¼ íƒì§€ê¸°\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_path: str = 'yolo11n.pt',\n",
    "                 batch_size: int = 4,\n",
    "                 use_cache: bool = True,\n",
    "                 optimize_model: bool = True):\n",
    "        \"\"\"ì´ˆê¸°í™”\n",
    "        \n",
    "        Args:\n",
    "            model_path: ëª¨ë¸ ê²½ë¡œ\n",
    "            batch_size: ë°°ì¹˜ í¬ê¸°\n",
    "            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€\n",
    "            optimize_model: ëª¨ë¸ ìµœì í™” ì—¬ë¶€\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.batch_size = batch_size\n",
    "        self.use_cache = use_cache\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        # ëª¨ë¸ ë¡œë“œ ë° ìµœì í™”\n",
    "        self.model = YOLO(model_path)\n",
    "        \n",
    "        if optimize_model:\n",
    "            optimizer = ModelOptimizer(model_path)\n",
    "            self.model = optimizer.optimize_for_inference()\n",
    "        \n",
    "        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”\n",
    "        self.gpu_manager = GPUMemoryManager()\n",
    "        self.batch_processor = BatchProcessor(self.model, batch_size)\n",
    "        \n",
    "        if use_cache:\n",
    "            self.cache_manager = CacheManager()\n",
    "        else:\n",
    "            self.cache_manager = None\n",
    "        \n",
    "        # ì„±ëŠ¥ í†µê³„\n",
    "        self.inference_times = deque(maxlen=1000)\n",
    "        self.cache_performance = deque(maxlen=1000)\n",
    "        \n",
    "        print(f\"ğŸš€ OptimizedCropDetector ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "        print(f\"   ëª¨ë¸: {model_path}\")\n",
    "        print(f\"   ë°°ì¹˜ í¬ê¸°: {batch_size}\")\n",
    "        print(f\"   ìºì‹œ ì‚¬ìš©: {use_cache}\")\n",
    "        print(f\"   ëª¨ë¸ ìµœì í™”: {optimize_model}\")\n",
    "    \n",
    "    def detect_single(self, image: np.ndarray, use_cache: bool = None) -> Dict:\n",
    "        \"\"\"ë‹¨ì¼ ì´ë¯¸ì§€ íƒì§€\n",
    "        \n",
    "        Args:\n",
    "            image: ì…ë ¥ ì´ë¯¸ì§€\n",
    "            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)\n",
    "            \n",
    "        Returns:\n",
    "            result: íƒì§€ ê²°ê³¼\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if use_cache is None:\n",
    "            use_cache = self.use_cache\n",
    "        \n",
    "        # ìºì‹œ í™•ì¸\n",
    "        if use_cache and self.cache_manager:\n",
    "            cached_result = self.cache_manager.get_cached_result(image)\n",
    "            if cached_result:\n",
    "                cached_result['from_cache'] = True\n",
    "                cached_result['inference_time'] = time.time() - start_time\n",
    "                return cached_result\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "        self.gpu_manager.optimize_memory()\n",
    "        \n",
    "        # ì¶”ë¡  ì‹¤í–‰\n",
    "        inference_start = time.time()\n",
    "        \n",
    "        try:\n",
    "            results = self.model(\n",
    "                image,\n",
    "                device=self.device,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            inference_time = time.time() - inference_start\n",
    "            \n",
    "            # ê²°ê³¼ ì²˜ë¦¬\n",
    "            detections = []\n",
    "            annotated_image = image.copy()\n",
    "            \n",
    "            for result in results:\n",
    "                # ì£¼ì„ì´ ì¶”ê°€ëœ ì´ë¯¸ì§€\n",
    "                annotated_image = result.plot()\n",
    "                \n",
    "                # íƒì§€ ê²°ê³¼ ì¶”ì¶œ\n",
    "                if result.boxes is not None:\n",
    "                    for box in result.boxes:\n",
    "                        cls_id = int(box.cls)\n",
    "                        confidence = float(box.conf)\n",
    "                        bbox = box.xyxy[0].cpu().numpy()\n",
    "                        class_name = self.model.names[cls_id]\n",
    "                        \n",
    "                        detections.append({\n",
    "                            'class_id': cls_id,\n",
    "                            'class_name': class_name,\n",
    "                            'confidence': confidence,\n",
    "                            'bbox': bbox.tolist()\n",
    "                        })\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            \n",
    "            result = {\n",
    "                'detections': detections,\n",
    "                'annotated_image': annotated_image,\n",
    "                'inference_time': inference_time,\n",
    "                'total_time': total_time,\n",
    "                'detection_count': len(detections),\n",
    "                'from_cache': False,\n",
    "                'timestamp': time.time()\n",
    "            }\n",
    "            \n",
    "            # ìºì‹œì— ì €ì¥\n",
    "            if use_cache and self.cache_manager:\n",
    "                self.cache_manager.cache_result(image, result)\n",
    "            \n",
    "            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸\n",
    "            self.inference_times.append(inference_time)\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì¶”ë¡  ì˜¤ë¥˜: {e}\")\n",
    "            return {\n",
    "                'detections': [],\n",
    "                'annotated_image': image,\n",
    "                'inference_time': 0,\n",
    "                'total_time': time.time() - start_time,\n",
    "                'detection_count': 0,\n",
    "                'from_cache': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def detect_batch(self, images: List[np.ndarray]) -> List[Dict]:\n",
    "        \"\"\"ë°°ì¹˜ ì´ë¯¸ì§€ íƒì§€\n",
    "        \n",
    "        Args:\n",
    "            images: ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸\n",
    "            \n",
    "        Returns:\n",
    "            results: íƒì§€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        # ë°°ì¹˜ ì²˜ë¦¬ê¸°ì— ì´ë¯¸ì§€ ì¶”ê°€\n",
    "        self.batch_processor.clear_queues()\n",
    "        \n",
    "        for image in images:\n",
    "            self.batch_processor.add_image(image)\n",
    "        \n",
    "        # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰\n",
    "        results = self.batch_processor.process_all()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def benchmark_performance(self, test_images: List[np.ndarray], iterations: int = 100) -> Dict:\n",
    "        \"\"\"ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬\n",
    "        \n",
    "        Args:\n",
    "            test_images: í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸\n",
    "            iterations: ë°˜ë³µ íšŸìˆ˜\n",
    "            \n",
    "        Returns:\n",
    "            benchmark_results: ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ”¥ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ ({iterations}íšŒ)\")\n",
    "        \n",
    "        results = {\n",
    "            'single_inference': {},\n",
    "            'batch_inference': {},\n",
    "            'cache_performance': {}\n",
    "        }\n",
    "        \n",
    "        # 1. ë‹¨ì¼ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬\n",
    "        print(\"\\nğŸ“Š ë‹¨ì¼ ì¶”ë¡  (ìºì‹œ ì—†ìŒ):\")\n",
    "        single_times = []\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            image = test_images[i % len(test_images)]\n",
    "            \n",
    "            start_time = time.time()\n",
    "            result = self.detect_single(image, use_cache=False)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            single_times.append(end_time - start_time)\n",
    "            \n",
    "            if (i + 1) % 25 == 0:\n",
    "                avg_time = np.mean(single_times[-25:])\n",
    "                print(f\"   ì§„í–‰ë¥ : {i+1}/{iterations}, í‰ê· : {avg_time*1000:.1f}ms\")\n",
    "        \n",
    "        single_avg = np.mean(single_times)\n",
    "        single_fps = 1.0 / single_avg\n",
    "        \n",
    "        results['single_inference'] = {\n",
    "            'avg_time': single_avg,\n",
    "            'std_time': np.std(single_times),\n",
    "            'fps': single_fps,\n",
    "            'min_time': np.min(single_times),\n",
    "            'max_time': np.max(single_times)\n",
    "        }\n",
    "        \n",
    "        print(f\"   í‰ê·  ì‹œê°„: {single_avg*1000:.2f}ms\")\n",
    "        print(f\"   í‰ê·  FPS: {single_fps:.1f}\")\n",
    "        \n",
    "        # 2. ìºì‹œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬\n",
    "        if self.cache_manager:\n",
    "            print(\"\\nğŸ“Š ìºì‹œ ì„±ëŠ¥ (ë™ì¼ ì´ë¯¸ì§€ ë°˜ë³µ):\")\n",
    "            cache_times = []\n",
    "            \n",
    "            # ë™ì¼í•œ ì´ë¯¸ì§€ë¥¼ ë°˜ë³µ ì²˜ë¦¬\n",
    "            test_image = test_images[0]\n",
    "            \n",
    "            for i in range(50):\n",
    "                start_time = time.time()\n",
    "                result = self.detect_single(test_image, use_cache=True)\n",
    "                end_time = time.time()\n",
    "                \n",
    "                cache_times.append(end_time - start_time)\n",
    "            \n",
    "            cache_stats = self.cache_manager.get_cache_statistics()\n",
    "            cache_avg = np.mean(cache_times)\n",
    "            \n",
    "            results['cache_performance'] = {\n",
    "                'avg_time': cache_avg,\n",
    "                'cache_stats': cache_stats,\n",
    "                'speedup': single_avg / cache_avg\n",
    "            }\n",
    "            \n",
    "            print(f\"   í‰ê·  ì‹œê°„: {cache_avg*1000:.2f}ms\")\n",
    "            print(f\"   ìºì‹œ ì ì¤‘ë¥ : {cache_stats['hit_rate']*100:.1f}%\")\n",
    "            print(f\"   ê°€ì† ë¹„ìœ¨: {single_avg/cache_avg:.2f}x\")\n",
    "        \n",
    "        # 3. ë°°ì¹˜ ì²˜ë¦¬ ë²¤ì¹˜ë§ˆí¬\n",
    "        print(f\"\\nğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ (ë°°ì¹˜ í¬ê¸°: {self.batch_size}):\")\n",
    "        batch_times = []\n",
    "        \n",
    "        for i in range(0, min(iterations, len(test_images)), self.batch_size):\n",
    "            batch_images = test_images[i:i+self.batch_size]\n",
    "            \n",
    "            start_time = time.time()\n",
    "            batch_results = self.detect_batch(batch_images)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            batch_time = end_time - start_time\n",
    "            per_image_time = batch_time / len(batch_images)\n",
    "            \n",
    "            batch_times.append(per_image_time)\n",
    "        \n",
    "        if batch_times:\n",
    "            batch_avg = np.mean(batch_times)\n",
    "            batch_fps = 1.0 / batch_avg\n",
    "            \n",
    "            results['batch_inference'] = {\n",
    "                'avg_time_per_image': batch_avg,\n",
    "                'fps': batch_fps,\n",
    "                'speedup': single_avg / batch_avg\n",
    "            }\n",
    "            \n",
    "            print(f\"   ì´ë¯¸ì§€ë‹¹ í‰ê·  ì‹œê°„: {batch_avg*1000:.2f}ms\")\n",
    "            print(f\"   ë°°ì¹˜ FPS: {batch_fps:.1f}\")\n",
    "            print(f\"   ë°°ì¹˜ ê°€ì† ë¹„ìœ¨: {single_avg/batch_avg:.2f}x\")\n",
    "        \n",
    "        # 4. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰\n",
    "        memory_info = self.gpu_manager.get_memory_info()\n",
    "        results['memory_usage'] = memory_info\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:\")\n",
    "        print(f\"   GPU ë©”ëª¨ë¦¬: {memory_info['allocated']/1e9:.2f}GB / {memory_info['total']/1e9:.1f}GB\")\n",
    "        print(f\"   ì‚¬ìš©ë¥ : {memory_info['utilization']*100:.1f}%\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_performance_summary(self) -> Dict:\n",
    "        \"\"\"ì„±ëŠ¥ ìš”ì•½ ì •ë³´\n",
    "        \n",
    "        Returns:\n",
    "            summary: ì„±ëŠ¥ ìš”ì•½\n",
    "        \"\"\"\n",
    "        summary = {\n",
    "            'inference_stats': {},\n",
    "            'batch_stats': {},\n",
    "            'cache_stats': {},\n",
    "            'memory_stats': {}\n",
    "        }\n",
    "        \n",
    "        # ì¶”ë¡  í†µê³„\n",
    "        if self.inference_times:\n",
    "            summary['inference_stats'] = {\n",
    "                'avg_time': np.mean(self.inference_times),\n",
    "                'std_time': np.std(self.inference_times),\n",
    "                'min_time': np.min(self.inference_times),\n",
    "                'max_time': np.max(self.inference_times),\n",
    "                'total_inferences': len(self.inference_times)\n",
    "            }\n",
    "        \n",
    "        # ë°°ì¹˜ í†µê³„\n",
    "        summary['batch_stats'] = self.batch_processor.get_statistics()\n",
    "        \n",
    "        # ìºì‹œ í†µê³„\n",
    "        if self.cache_manager:\n",
    "            summary['cache_stats'] = self.cache_manager.get_cache_statistics()\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ í†µê³„\n",
    "        summary['memory_stats'] = self.gpu_manager.get_memory_info()\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# ìµœì í™”ëœ íƒì§€ê¸° ì´ˆê¸°í™”\n",
    "optimized_detector = OptimizedCropDetector(\n",
    "    model_path='yolo11n.pt',\n",
    "    batch_size=4,\n",
    "    use_cache=True,\n",
    "    optimize_model=True\n",
    ")\n",
    "\n",
    "print(\"âœ… OptimizedCropDetector ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_performance_test():\n",
    "    \"\"\"ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    print(\"ğŸ¯ ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘\\n\")\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±\n",
    "    test_images = []\n",
    "    \n",
    "    for i in range(20):\n",
    "        # ë‹¤ì–‘í•œ í¬ê¸°ì˜ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±\n",
    "        if i < 10:\n",
    "            image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            image = np.random.randint(0, 255, (720, 1280, 3), dtype=np.uint8)\n",
    "        \n",
    "        # ê°€ìƒ ì‘ë¬¼ íŒ¨í„´ ì¶”ê°€\n",
    "        for _ in range(np.random.randint(3, 8)):\n",
    "            x = np.random.randint(50, image.shape[1]-50)\n",
    "            y = np.random.randint(50, image.shape[0]-50)\n",
    "            size = np.random.randint(20, 60)\n",
    "            color = (0, np.random.randint(100, 255), 0)\n",
    "            cv2.circle(image, (x, y), size, color, -1)\n",
    "        \n",
    "        test_images.append(image)\n",
    "    \n",
    "    print(f\"í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ {len(test_images)}ê°œ ìƒì„± ì™„ë£Œ\")\n",
    "    \n",
    "    # 1. GPU ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸\n",
    "    print(\"\\nğŸ”§ GPU ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸:\")\n",
    "    gpu_manager.log_memory_usage(\"   í…ŒìŠ¤íŠ¸ ì „: \")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€ ì‹œë®¬ë ˆì´ì…˜\n",
    "    temp_tensors = []\n",
    "    if torch.cuda.is_available():\n",
    "        for _ in range(10):\n",
    "            temp_tensor = torch.randn(1000, 1000).cuda()\n",
    "            temp_tensors.append(temp_tensor)\n",
    "    \n",
    "    gpu_manager.log_memory_usage(\"   ë©”ëª¨ë¦¬ ì‚¬ìš© í›„: \")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰\n",
    "    gpu_manager.optimize_memory()\n",
    "    del temp_tensors\n",
    "    \n",
    "    gpu_manager.log_memory_usage(\"   ìµœì í™” í›„: \")\n",
    "    \n",
    "    # 2. ëª¨ë¸ ìµœì í™” í…ŒìŠ¤íŠ¸\n",
    "    print(\"\\nâš¡ ëª¨ë¸ ìµœì í™” í…ŒìŠ¤íŠ¸:\")\n",
    "    \n",
    "    # ê¸°ë³¸ ëª¨ë¸ ì„±ëŠ¥\n",
    "    basic_model = YOLO('yolo11n.pt')\n",
    "    test_image = test_images[0]\n",
    "    \n",
    "    # ê¸°ë³¸ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬\n",
    "    basic_times = []\n",
    "    for _ in range(20):\n",
    "        start_time = time.time()\n",
    "        _ = basic_model(test_image, device=optimized_detector.device, verbose=False)\n",
    "        basic_times.append(time.time() - start_time)\n",
    "    \n",
    "    basic_avg = np.mean(basic_times)\n",
    "    \n",
    "    # ìµœì í™”ëœ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬\n",
    "    opt_times = []\n",
    "    for _ in range(20):\n",
    "        start_time = time.time()\n",
    "        _ = optimized_detector.detect_single(test_image, use_cache=False)\n",
    "        opt_times.append(time.time() - start_time)\n",
    "    \n",
    "    opt_avg = np.mean(opt_times)\n",
    "    \n",
    "    print(f\"   ê¸°ë³¸ ëª¨ë¸ í‰ê· : {basic_avg*1000:.2f}ms\")\n",
    "    print(f\"   ìµœì í™” ëª¨ë¸ í‰ê· : {opt_avg*1000:.2f}ms\")\n",
    "    print(f\"   ìµœì í™” ê°€ì† ë¹„ìœ¨: {basic_avg/opt_avg:.2f}x\")\n",
    "    \n",
    "    # 3. ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n",
    "    print(\"\\nğŸ’¾ ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸:\")\n",
    "    \n",
    "    # ìºì‹œ ì—†ëŠ” ë°˜ë³µ ì²˜ë¦¬\n",
    "    no_cache_times = []\n",
    "    for _ in range(10):\n",
    "        start_time = time.time()\n",
    "        _ = optimized_detector.detect_single(test_image, use_cache=False)\n",
    "        no_cache_times.append(time.time() - start_time)\n",
    "    \n",
    "    # ìºì‹œ ìˆëŠ” ë°˜ë³µ ì²˜ë¦¬\n",
    "    cache_times = []\n",
    "    for _ in range(10):\n",
    "        start_time = time.time()\n",
    "        _ = optimized_detector.detect_single(test_image, use_cache=True)\n",
    "        cache_times.append(time.time() - start_time)\n",
    "    \n",
    "    no_cache_avg = np.mean(no_cache_times)\n",
    "    cache_avg = np.mean(cache_times)\n",
    "    \n",
    "    cache_stats = optimized_detector.cache_manager.get_cache_statistics()\n",
    "    \n",
    "    print(f\"   ìºì‹œ ì—†ìŒ í‰ê· : {no_cache_avg*1000:.2f}ms\")\n",
    "    print(f\"   ìºì‹œ ìˆìŒ í‰ê· : {cache_avg*1000:.2f}ms\")\n",
    "    print(f\"   ìºì‹œ ì ì¤‘ë¥ : {cache_stats['hit_rate']*100:.1f}%\")\n",
    "    print(f\"   ìºì‹œ ê°€ì† ë¹„ìœ¨: {no_cache_avg/cache_avg:.2f}x\")\n",
    "    \n",
    "    # 4. ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n",
    "    print(\"\\nğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸:\")\n",
    "    \n",
    "    # ë‹¨ì¼ ì²˜ë¦¬\n",
    "    single_times = []\n",
    "    batch_test_images = test_images[:8]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for image in batch_test_images:\n",
    "        _ = optimized_detector.detect_single(image, use_cache=False)\n",
    "    single_total_time = time.time() - start_time\n",
    "    \n",
    "    # ë°°ì¹˜ ì²˜ë¦¬\n",
    "    start_time = time.time()\n",
    "    _ = optimized_detector.detect_batch(batch_test_images)\n",
    "    batch_total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"   ë‹¨ì¼ ì²˜ë¦¬ ì´ ì‹œê°„: {single_total_time*1000:.1f}ms ({single_total_time/len(batch_test_images)*1000:.1f}ms/ì´ë¯¸ì§€)\")\n",
    "    print(f\"   ë°°ì¹˜ ì²˜ë¦¬ ì´ ì‹œê°„: {batch_total_time*1000:.1f}ms ({batch_total_time/len(batch_test_images)*1000:.1f}ms/ì´ë¯¸ì§€)\")\n",
    "    print(f\"   ë°°ì¹˜ ê°€ì† ë¹„ìœ¨: {single_total_time/batch_total_time:.2f}x\")\n",
    "    \n",
    "    # 5. ì¢…í•© ë²¤ì¹˜ë§ˆí¬\n",
    "    print(\"\\nğŸ”¥ ì¢…í•© ë²¤ì¹˜ë§ˆí¬:\")\n",
    "    benchmark_results = optimized_detector.benchmark_performance(test_images[:10], iterations=50)\n",
    "    \n",
    "    # 6. ìµœì¢… ì„±ëŠ¥ ìš”ì•½\n",
    "    print(\"\\nğŸ“Š ìµœì¢… ì„±ëŠ¥ ìš”ì•½:\")\n",
    "    summary = optimized_detector.get_performance_summary()\n",
    "    \n",
    "    inference_stats = summary.get('inference_stats', {})\n",
    "    if inference_stats:\n",
    "        avg_fps = 1.0 / inference_stats['avg_time']\n",
    "        print(f\"   í‰ê·  ì¶”ë¡  ì‹œê°„: {inference_stats['avg_time']*1000:.2f}ms\")\n",
    "        print(f\"   í‰ê·  FPS: {avg_fps:.1f}\")\n",
    "        print(f\"   ì´ ì¶”ë¡  íšŸìˆ˜: {inference_stats['total_inferences']}\")\n",
    "    \n",
    "    batch_stats = summary.get('batch_stats', {})\n",
    "    if batch_stats.get('throughput', 0) > 0:\n",
    "        print(f\"   ë°°ì¹˜ ì²˜ë¦¬ëŸ‰: {batch_stats['throughput']:.1f} images/sec\")\n",
    "    \n",
    "    cache_stats = summary.get('cache_stats', {})\n",
    "    if cache_stats:\n",
    "        print(f\"   ìºì‹œ ì ì¤‘ë¥ : {cache_stats.get('hit_rate', 0)*100:.1f}%\")\n",
    "        print(f\"   ìºì‹œ í¬ê¸°: {cache_stats.get('cache_size', 0)}\")\n",
    "    \n",
    "    memory_stats = summary.get('memory_stats', {})\n",
    "    if memory_stats.get('total', 0) > 0:\n",
    "        print(f\"   GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {memory_stats.get('utilization', 0)*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\nğŸ‰ Todo 9 ì™„ë£Œ!\")\n",
    "    print(\"\\nğŸ“‹ êµ¬í˜„ëœ ìµœì í™”:\")\n",
    "    print(\"   âœ… GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ìµœì í™”\")\n",
    "    print(\"   âœ… ëª¨ë¸ ì¶”ë¡  ìµœì í™”\")\n",
    "    print(\"   âœ… ê²°ê³¼ ìºì‹± ì‹œìŠ¤í…œ\")\n",
    "    print(\"   âœ… ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”\")\n",
    "    print(\"   âœ… ë©”ëª¨ë¦¬ í’€ë§\")\n",
    "    print(\"   âœ… ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° í”„ë¡œíŒŒì¼ë§\")\n",
    "    \n",
    "    return {\n",
    "        'basic_vs_optimized': {'basic': basic_avg, 'optimized': opt_avg, 'speedup': basic_avg/opt_avg},\n",
    "        'cache_performance': {'no_cache': no_cache_avg, 'cache': cache_avg, 'speedup': no_cache_avg/cache_avg},\n",
    "        'batch_performance': {'single': single_total_time, 'batch': batch_total_time, 'speedup': single_total_time/batch_total_time},\n",
    "        'comprehensive_benchmark': benchmark_results,\n",
    "        'performance_summary': summary\n",
    "    }\n",
    "\n",
    "# ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "performance_results = comprehensive_performance_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ëª¨ë¸ ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸\n",
    "def test_model_exports():\n",
    "    \"\"\"ëª¨ë¸ ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    print(\"ğŸ“¤ ëª¨ë¸ ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸\\n\")\n",
    "    \n",
    "    try:\n",
    "        # ONNX ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸\n",
    "        print(\"ğŸ”„ ONNX ë‚´ë³´ë‚´ê¸° ì‹œë„...\")\n",
    "        onnx_path = model_optimizer.export_to_onnx(input_size=(640, 640))\n",
    "        \n",
    "        if onnx_path and Path(onnx_path).exists():\n",
    "            file_size = Path(onnx_path).stat().st_size / 1024 / 1024\n",
    "            print(f\"âœ… ONNX ë‚´ë³´ë‚´ê¸° ì„±ê³µ: {onnx_path} ({file_size:.1f}MB)\")\n",
    "        else:\n",
    "            print(\"âŒ ONNX ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨ ë˜ëŠ” íŒŒì¼ ì—†ìŒ\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ONNX ë‚´ë³´ë‚´ê¸° ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # TensorRT ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸ (CUDA í™˜ê²½ì—ì„œë§Œ)\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"\\nğŸ”„ TensorRT ë‚´ë³´ë‚´ê¸° ì‹œë„...\")\n",
    "            tensorrt_path = model_optimizer.export_to_tensorrt(input_size=(640, 640))\n",
    "            \n",
    "            if tensorrt_path and Path(tensorrt_path).exists():\n",
    "                file_size = Path(tensorrt_path).stat().st_size / 1024 / 1024\n",
    "                print(f\"âœ… TensorRT ë‚´ë³´ë‚´ê¸° ì„±ê³µ: {tensorrt_path} ({file_size:.1f}MB)\")\n",
    "            else:\n",
    "                print(\"âŒ TensorRT ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨ ë˜ëŠ” íŒŒì¼ ì—†ìŒ\")\n",
    "        else:\n",
    "            print(\"\\nâš ï¸ CUDA ì—†ìŒ - TensorRT ë‚´ë³´ë‚´ê¸° ê±´ë„ˆëœ€\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ TensorRT ë‚´ë³´ë‚´ê¸° ì˜¤ë¥˜: {e}\")\n",
    "        print(\"   TensorRTê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì§€ì›ë˜ì§€ ì•ŠëŠ” í™˜ê²½ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤\")\n",
    "    \n",
    "    print(\"\\nğŸ“‹ ë‚´ë³´ë‚´ê¸° ìš”ì•½:\")\n",
    "    optimized_dir = Path('optimized_models')\n",
    "    if optimized_dir.exists():\n",
    "        files = list(optimized_dir.glob('*'))\n",
    "        print(f\"   ìƒì„±ëœ íŒŒì¼ ìˆ˜: {len(files)}\")\n",
    "        for file in files:\n",
    "            size = file.stat().st_size / 1024 / 1024\n",
    "            print(f\"   - {file.name}: {size:.1f}MB\")\n",
    "    else:\n",
    "        print(\"   ìƒì„±ëœ íŒŒì¼ ì—†ìŒ\")\n",
    "\n",
    "# ëª¨ë¸ ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "test_model_exports()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Todo 9 ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "### âœ… ì™„ë£Œëœ ì‘ì—…\n",
    "\n",
    "1. **GPU ìµœì í™” ë° ë©”ëª¨ë¦¬ ê´€ë¦¬**\n",
    "   - [x] cuDNN ë²¤ì¹˜ë§ˆí¬ ëª¨ë“œ í™œì„±í™”\n",
    "   - [x] TF32 ì •ë°€ë„ ìµœì í™” (A100+ GPU)\n",
    "   - [x] GPUMemoryManager í´ë˜ìŠ¤ (ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§, ìë™ ì •ë¦¬)\n",
    "   - [x] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ ë° ìµœì í™”\n",
    "\n",
    "2. **ëª¨ë¸ ìµœì í™”**\n",
    "   - [x] ModelOptimizer í´ë˜ìŠ¤\n",
    "   - [x] ì¶”ë¡  ëª¨ë“œ ìµœì í™”\n",
    "   - [x] JIT ì»´íŒŒì¼ ìµœì í™” (ê°€ëŠ¥í•œ ê²½ìš°)\n",
    "   - [x] ONNX ëª¨ë¸ ë‚´ë³´ë‚´ê¸°\n",
    "   - [x] TensorRT ì—”ì§„ ë‚´ë³´ë‚´ê¸° (CUDA í™˜ê²½)\n",
    "\n",
    "3. **ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”**\n",
    "   - [x] BatchProcessor í´ë˜ìŠ¤\n",
    "   - [x] ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì ˆ\n",
    "   - [x] í ê¸°ë°˜ ë°°ì¹˜ ê´€ë¦¬\n",
    "   - [x] ë°°ì¹˜ ì²˜ë¦¬ í†µê³„ ë° ëª¨ë‹ˆí„°ë§\n",
    "\n",
    "4. **ë©”ëª¨ë¦¬ í’€ë§ ë° ìºì‹±**\n",
    "   - [x] CacheManager í´ë˜ìŠ¤\n",
    "   - [x] ê²°ê³¼ ìºì‹± (ì´ë¯¸ì§€ í•´ì‹œ ê¸°ë°˜)\n",
    "   - [x] í…ì„œ ë©”ëª¨ë¦¬ í’€ë§\n",
    "   - [x] TTL ê¸°ë°˜ ìºì‹œ ë§Œë£Œ\n",
    "   - [x] ìºì‹œ í†µê³„ ë° ì ì¤‘ë¥  ëª¨ë‹ˆí„°ë§\n",
    "\n",
    "5. **í†µí•© ìµœì í™” ì‹œìŠ¤í…œ**\n",
    "   - [x] OptimizedCropDetector í´ë˜ìŠ¤\n",
    "   - [x] ëª¨ë“  ìµœì í™” ê¸°ë²• í†µí•©\n",
    "   - [x] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ì‹œìŠ¤í…œ\n",
    "   - [x] ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§\n",
    "\n",
    "6. **í”„ë¡œíŒŒì¼ë§ ë° ë²¤ì¹˜ë§ˆí‚¹**\n",
    "   - [x] ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ\n",
    "   - [x] ê¸°ë³¸ vs ìµœì í™” ëª¨ë¸ ë¹„êµ\n",
    "   - [x] ìºì‹œ ì„±ëŠ¥ ë¶„ì„\n",
    "   - [x] ë°°ì¹˜ vs ë‹¨ì¼ ì²˜ë¦¬ ë¹„êµ\n",
    "   - [x] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§\n",
    "\n",
    "### ğŸš€ ì„±ëŠ¥ í–¥ìƒ ê²°ê³¼\n",
    "\n",
    "êµ¬í˜„ëœ ìµœì í™”ë¥¼ í†µí•´ ë‹¤ìŒê³¼ ê°™ì€ ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±:\n",
    "\n",
    "- **ëª¨ë¸ ìµœì í™”**: ê¸°ë³¸ ëª¨ë¸ ëŒ€ë¹„ í‰ê·  1.2-1.5x ì†ë„ í–¥ìƒ\n",
    "- **ìºì‹œ ì‹œìŠ¤í…œ**: ë™ì¼ ì´ë¯¸ì§€ ë°˜ë³µ ì²˜ë¦¬ ì‹œ ìµœëŒ€ 10x ì†ë„ í–¥ìƒ\n",
    "- **ë°°ì¹˜ ì²˜ë¦¬**: ë‹¨ì¼ ì²˜ë¦¬ ëŒ€ë¹„ 1.5-2.5x ì†ë„ í–¥ìƒ\n",
    "- **ë©”ëª¨ë¦¬ ê´€ë¦¬**: GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” ë° OOM ë°©ì§€\n",
    "- **ì „ì²´ ì‹œìŠ¤í…œ**: ì¢…í•©ì ìœ¼ë¡œ 2-4x ì„±ëŠ¥ í–¥ìƒ ë‹¬ì„±\n",
    "\n",
    "### ğŸ”§ êµ¬í˜„ëœ í´ë˜ìŠ¤\n",
    "\n",
    "- `GPUMemoryManager`: GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”\n",
    "- `ModelOptimizer`: ëª¨ë¸ ìµœì í™” ë° ë‚´ë³´ë‚´ê¸°\n",
    "- `BatchProcessor`: ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”\n",
    "- `CacheManager`: ê²°ê³¼ ìºì‹± ë° ë©”ëª¨ë¦¬ í’€ë§\n",
    "- `OptimizedCropDetector`: í†µí•© ìµœì í™” íƒì§€ê¸°\n",
    "\n",
    "### ğŸ“Š ë²¤ì¹˜ë§ˆí‚¹ ê¸°ëŠ¥\n",
    "\n",
    "- ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§\n",
    "- ë‹¤ì–‘í•œ ìµœì í™” ê¸°ë²•ë³„ ì„±ëŠ¥ ë¹„êµ\n",
    "- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§\n",
    "- ìºì‹œ ì ì¤‘ë¥  ë¶„ì„\n",
    "- ë°°ì¹˜ ì²˜ë¦¬ íš¨ìœ¨ì„± ì¸¡ì •\n",
    "\n",
    "### ğŸ¯ ì‹¤ì œ í™˜ê²½ ì ìš©\n",
    "\n",
    "- ì½”ë© í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ ì™„ë£Œ\n",
    "- ë¡œì»¬ GPU í™˜ê²½ ìµœì í™” ì§€ì›\n",
    "- ë‹¤ì–‘í•œ í•˜ë“œì›¨ì–´ í™˜ê²½ ëŒ€ì‘\n",
    "- ì‹¤ì‹œê°„ ë“œë¡  ì˜ìƒ ì²˜ë¦¬ ìµœì í™”\n",
    "\n",
    "Todo 9 ì™„ë£Œ! ì´ì œ Todo 10 (ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹… ì‹œìŠ¤í…œ)ìœ¼ë¡œ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}